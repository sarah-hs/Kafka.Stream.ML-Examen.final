{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"name":"KafkaClient(Consumer).ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"2ZwRs2i8WVnf"},"source":["!pip install pyspark==2.4.5\n","!pip install lightning-python"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pf50PDfsCTfv","executionInfo":{"status":"ok","timestamp":1606172900690,"user_tz":360,"elapsed":1114,"user":{"displayName":"Sarah Hernández Serrano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPu56UtRlNcYlVlUouF6yEgbR8E4j0MVtAhTnKayY=s64","userId":"18255259528200381895"}}},"source":["import os\n","# os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars spark-streaming-kafka-0-8-assembly_2.11-2.4.5.jar pyspark-shell' \n","# Downloaded from: https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka-0-8-assembly_2.11/2.4.5\n","os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2 pyspark-shell'\n","\n","from pyspark import SparkContext\n","from pyspark.streaming import StreamingContext\n","from pyspark.streaming.kafka import KafkaUtils\n","from time import sleep\n","from lightning import Lightning\n","import json\n","import numpy as np\n","import requests"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CJlLLGSjCTfw"},"source":["#### Init Lightning"]},{"cell_type":"code","metadata":{"id":"1u8PENGaCTfw","outputId":"7135078b-eb63-4688-95a6-9f60c1b765ba"},"source":["# Init LIGHTNING-VIZ server\n","lgn = Lightning(host=\"http://localhost:3000\")\n","lgn.create_session(\"Rojo\")\n","\n","series = np.random.randn(3,1)\n","viz = lgn.linestreaming(series)\n","print(series)\n","#x['location']['name'], "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Lightning initialized\n","Connected to server at http://localhost:3000\n","[[-0.18667612]\n"," [-0.43275402]\n"," [-1.40955176]\n"," [-0.82131372]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NlQ20SWnCTfw"},"source":["#### Init StramingContext and KafkaDirectStream"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":341},"id":"vcHgk6lmCTfw","executionInfo":{"status":"error","timestamp":1606173655484,"user_tz":360,"elapsed":1493,"user":{"displayName":"Sarah Hernández Serrano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPu56UtRlNcYlVlUouF6yEgbR8E4j0MVtAhTnKayY=s64","userId":"18255259528200381895"}},"outputId":"998f3099-cbd6-4a62-df45-49fe062894a5"},"source":["sc = SparkContext(appName=\"ExamenFinal\")\n","ssc = StreamingContext(sc, 2)\n","ssc.checkpoint('/tmp')"],"execution_count":14,"outputs":[{"output_type":"error","ename":"Py4JError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-c41c0bbfb115>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ExamenFinal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mssc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStreamingContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tmp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 136\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;31m# data via a socket.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# scala's mangled names w/ $ in them require special treatment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encryption_enabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetEncryptionEnabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpythonExec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PYSPARK_PYTHON\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mUserHelpAutoCompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPy4JError\u001b[0m: org.apache.spark.api.python.PythonUtils.isEncryptionEnabled does not exist in the JVM"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":198},"id":"7R124iSJCTfx","executionInfo":{"status":"error","timestamp":1606172911051,"user_tz":360,"elapsed":465,"user":{"displayName":"Sarah Hernández Serrano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPu56UtRlNcYlVlUouF6yEgbR8E4j0MVtAhTnKayY=s64","userId":"18255259528200381895"}},"outputId":"4617e384-915c-4037-b28f-e81c20b07f2a"},"source":["topic_list = [\"a1cp94e3-weather\"]\n","brokers = {'metadata.broker.list': 'localhost:9092'}\n","\n","kafkaParams_dict = {\n","    'metadata.broker.list': 'tricycle-01.srvs.cloudkafka.com:9094,tricycle-02.srvs.cloudkafka.com:9094,tricycle-03.srvs.cloudkafka.com:9094',\n","    'sasl.mechanism': 'SCRAM-SHA-256',\n","    'security.protocol': 'SASL_SSL',\n","    'sasl.username': 'a1cp94e3',\n","    'sasl.password': 'W8vGwbGkvoL1_ubC-SF4BhpETpVGTpyc',\n","    'group.id': 'a1cp94e3-consumer',\n","    'session.timeout.ms': 6000,\n","    'default.topic.config': {'auto.offset.reset': 'smallest'},\n","    'ssl.ca.location': 'cloudkarafka.ca'\n","}\n","\n","#kafkaStream = KafkaUtils.createDirectStream(ssc, topic_list, brokers)\n","kafkaStream = KafkaUtils.createDirectStream(ssc, topics = topic_list, kafkaParams = kafkaParams_dict)"],"execution_count":12,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-807642734277>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#kafkaStream = KafkaUtils.createDirectStream(ssc, topic_list, brokers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mkafkaStream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKafkaUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDirectStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkafkaParams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkafkaParams_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'ssc' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"lOvBZq1ZCTfx"},"source":["#### Streaming"]},{"cell_type":"code","metadata":{"id":"Y3mHmOqUCTfx","outputId":"e14c502c-848f-46a2-fefc-15e4efd8cfc6"},"source":["jsons = kafkaStream.map(lambda x: json.loads(x[1]))\n","\n","jsons1 = jsons.map(lambda x: (x['current']['weather_descriptions'][0], x['current']['temperature'])).groupByKey().reduceByKey(lambda x,y: x+y).map(lambda x: (x[0], len(x[1])))\n","jsons1.pprint()\n","\n","def measures(data):\n","    return data[1]\n","    \n","def sendViz(rdd):\n","    ll = rdd.foreach(measures)\n","    arr = ll.collect()\n","    arr.pprint()\n","    viz.append(np.array(arr))\n","    return arr\n","\n","#plot = jsons.map(lambda x: measures(x, cols))\n","#plot.foreachRDD(lambda rdd: rdd.foreach(sendRecord))\n","\n","ssc.start()\n","ssc.awaitTermination(700)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"An error occurred while calling o24.awaitTerminationOrTimeout.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/Users/sarah/opt/miniconda3/lib/python3.7/site-packages/pyspark/streaming/util.py\", line 68, in call\n    r = self.func(t, *rdds)\n  File \"/Users/sarah/opt/miniconda3/lib/python3.7/site-packages/pyspark/streaming/dstream.py\", line 161, in <lambda>\n    func = lambda t, rdd: old_func(rdd)\n  File \"<ipython-input-5-7bd1039beef2>\", line 16, in <lambda>\n    jsons1.foreachRDD(lambda rdd: sendViz(rdd))\n  File \"<ipython-input-5-7bd1039beef2>\", line 11, in sendViz\n    arr = ll.collect()\nAttributeError: 'NoneType' object has no attribute 'collect'\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-7bd1039beef2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o24.awaitTerminationOrTimeout.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/Users/sarah/opt/miniconda3/lib/python3.7/site-packages/pyspark/streaming/util.py\", line 68, in call\n    r = self.func(t, *rdds)\n  File \"/Users/sarah/opt/miniconda3/lib/python3.7/site-packages/pyspark/streaming/dstream.py\", line 161, in <lambda>\n    func = lambda t, rdd: old_func(rdd)\n  File \"<ipython-input-5-7bd1039beef2>\", line 16, in <lambda>\n    jsons1.foreachRDD(lambda rdd: sendViz(rdd))\n  File \"<ipython-input-5-7bd1039beef2>\", line 11, in sendViz\n    arr = ll.collect()\nAttributeError: 'NoneType' object has no attribute 'collect'\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"]}]},{"cell_type":"code","metadata":{"id":"KVsuRUxBCTfx"},"source":["ssc.stop(stopSparkContext=True)\n","sc.stop()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J_cEiusgCTfx"},"source":["conf = {\n","    'bootstrap.servers': 'tricycle-01.srvs.cloudkafka.com:9094,tricycle-02.srvs.cloudkafka.com:9094,tricycle-03.srvs.cloudkafka.com:9094',\n","    'group_id': 'a1cp94e3-consumer',\n","    'request_timeout_ms': 6000,\n","    #'default.topic.config': {'auto.offset.reset': 'smallest'},\n","    'security_protocol': 'SASL_SSL',\n","    'sasl_mechanism': 'SCRAM-SHA-256',\n","    'sasl_plain_username': 'a1cp94e3',\n","    'sasl_plain_password': 'W8vGwbGkvoL1_ubC-SF4BhpETpVGTpyc',\n","    'ssl_cafile': 'cloudkarafka.ca'\n","}"],"execution_count":null,"outputs":[]}]}